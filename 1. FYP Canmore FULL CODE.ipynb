{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607fdbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the packages that we will need when working with dataframes and geometry data\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from shapely.geometry import Point, Polygon\n",
    "import numpy as np\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from shapely.geometry.multipolygon import MultiPolygon\n",
    "from pandas.api.types import is_string_dtype\n",
    "from pandas.api.types import is_numeric_dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d299b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### set the location of the files - keep in mind these are on L drive so need to be connected to the VPN\n",
    "\n",
    "path = \"L:/EMPoWeR Pilot Study/Data/GPS Data/410002/Canmore/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76fa930",
   "metadata": {},
   "outputs": [],
   "source": [
    "##remove the files if they exist as they will cause problems when running the script multiple times\n",
    "if os.path.exists(os.path.join(path, \"410002_all_points_unclean.csv\")):\n",
    "    os.remove(os.path.join(path, \"410002_all_points_unclean.csv\"))\n",
    "if os.path.exists(os.path.join(path, \"410002_allpoints_clean.csv\")):\n",
    "    os.remove(os.path.join(path, \"410002_allpoints_clean.csv\"))\n",
    "if os.path.exists(os.path.join(path, \"410002_allpoints_clean_final.csv\")):\n",
    "    os.remove(os.path.join(path, \"410002_allpoints_clean_final.csv\"))\n",
    "if os.path.exists(os.path.join(path, \"410002_sleep_points.csv\")):\n",
    "    os.remove(os.path.join(path, \"410002_sleep_points.csv\"))\n",
    "if os.path.exists(os.path.join(path, \"410002_totalcounts.csv\")):\n",
    "    os.remove(os.path.join(path, \"410002_totalcounts.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c05aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a list of all the files in the folder\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83923372",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#read all the files into the dataframe\n",
    "#if running code over and over, need to only have the path to just the separated gps files\n",
    "df = pd.concat((pd.read_csv(f, header=None) for f in all_files)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0347ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These data don't have headers already so lets make some\n",
    "headers = ['index1','index', 'date(UTC)', 'time(UTC)', 'date(local)', 'time(local)', 'latitude', 'lat(DMM)', 'wrong_longitude', 'lon(DMM)', 'elevation(m)', 'speed(km)']\n",
    "df.columns = headers\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd081ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all the longitudes need to be negative so let's convert by slicing\n",
    "#this is bc Canmore data are recorded in degrees minutes seconds (N/W) and need to be in decimal degrees (-long)\n",
    "#first convert the long column to float if it isn't already\n",
    "df['wrong_longitude'] = df['wrong_longitude'].astype(float)\n",
    "\n",
    "#create a new column with the longitude converted to negative \n",
    "df['longitude'] = (df.loc[:,'wrong_longitude']) * (-1)\n",
    "\n",
    "\n",
    "#lets get rid of all these unecessary columns that we don't care about\n",
    "df = df.drop(columns=['index1', 'lat(DMM)', 'wrong_longitude', 'lon(DMM)'])\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b603eb98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#combine date(local) & time(local) into a new column called DateTime_Local and then convert to datetime object\n",
    "df['DateTime_Local'] =  pd.to_datetime(df['date(local)'].astype(str)+' '+ df['time(local)'].astype(str))\n",
    "df['DateTime_Local'] = pd.to_datetime(df['DateTime_Local'])\n",
    "\n",
    "#now lets just move it over after all the other times and move longitude next to latitude to make it look nice\n",
    "column_to_move = df.pop(\"DateTime_Local\")\n",
    "df.insert(5, \"DateTime_Local\", column_to_move)\n",
    "\n",
    "column_to_move = df.pop(\"longitude\")\n",
    "df.insert(7, \"longitude\", column_to_move)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fcf26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select only the dates that match with the EMA dataset dates. Can be found on \"GPS Matching\" Excel\n",
    "#need to use the following code with the index\n",
    "df.set_index('DateTime_Local', inplace=True)\n",
    "\n",
    "# slice the data \n",
    "From = '2022-04-13'\n",
    "To   = '2022-04-26'\n",
    "\n",
    "df = df.loc[From:To,:]\n",
    "\n",
    "#save to the file\n",
    "df.to_csv(os.path.join(path, '410002_all_points_unclean.csv'))\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e0027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run Haversine fcn to get the euclidean distance between data points and store in new column calculating distance in km and mi\n",
    "def haversine(lat1, lon1, lat2, lon2, to_radians=True, earth_radius=6371):\n",
    "    \"\"\"\n",
    "    slightly modified version: of http://stackoverflow.com/a/29546836/2901002\n",
    "\n",
    "    Calculate the great circle distance between two points\n",
    "    on the earth (specified in decimal degrees or in radians)\n",
    "\n",
    "    All (lat, lon) coordinates must have numeric dtypes and be of equal length.\n",
    "\n",
    "    \"\"\"\n",
    "    if to_radians:\n",
    "        lat1, lon1, lat2, lon2 = map(np.radians,[lat1,lon1,lat2,lon2])\n",
    "\n",
    "    a = np.sin((lat2-lat1)/2.0)**2 + \\\n",
    "        np.cos(lat1) * np.cos(lat2) * np.sin((lon2-lon1)/2.0)**2\n",
    "\n",
    "    return earth_radius * 2 * np.arcsin(np.sqrt(a))\n",
    "\n",
    "\n",
    "df['distKM'] = \\\n",
    "    haversine(df.latitude.shift(), df.longitude.shift(),\n",
    "              df.loc[1:, 'latitude'], df.loc[1:, 'longitude'])\n",
    "df['distMI'] = df['distKM'] * 0.621371\n",
    "#distKM is in km, distMI is converted to miles\n",
    "#can double check values for code working with this calculator:https://www.calculator.net/distance-calculator.html\n",
    "#note that lat lon decimal points round to the 6th decimal point;6th decimal point measures to up to 0.11m error\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec9bf7c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#calculate the difference in time from each previous timestamp in hh:mm:ss and put in column called 'diff_in_hours'\n",
    "\n",
    "#make the DateTime_Local the index to run to_series function\n",
    "df.index = df['DateTime_Local']\n",
    "\n",
    "#now calculate distance from previous timepoint to next in hour format\n",
    "df['diff_in_hours'] = df.index.to_series().diff()\n",
    "\n",
    "#calculate new columns for time between points in mins and secs \n",
    "df['durationSecs'] = df['diff_in_hours'].dt.total_seconds()\n",
    "df['durationMins'] = df['durationSecs'] // 60\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5bd9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downsample the Canmore 5s data to 20s\n",
    "df = df.resample('20S').first()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ffe564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates empty rows when resampling, so need to drop these first \n",
    "df['index'].replace('', np.nan, inplace=True)\n",
    "df.dropna(subset=['index'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4967f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this tells us how many rows we have left after downsampling and removing those empty rows\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac40d1b0",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "Now we will pre-process the data by finding erroneous points first by applying our filtering criteria\n",
    "-We will make sure that speed is <95mph (152.888km/hr)\n",
    "-We will make sure that elevation is <14410 feet (highest hiking spot in USA)(4392 meters)\n",
    "\n",
    "-Note that speed is in km in our dataset & elevation in meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d983620",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new column where we will identify if a row is erroneous\n",
    "df['noise'] = np.nan\n",
    "\n",
    "#convert both speed and elevation to integers & make the name simpler or following code won't work\n",
    "df['speed'] = df['speed(km)'].astype(int)\n",
    "df['elevation'] = df['elevation(m)'].astype(int)\n",
    "\n",
    "#drop the old column names because its getting messy\n",
    "df = df.drop(columns=['speed(km)', 'elevation(m)'])\n",
    "\n",
    "#find where speed is > than our criteria, if it finds one, it will mark '1' in the noise column\n",
    "df.loc[df.speed >= 152, 'noise'] = 1\n",
    "#find where elevation is > than our criteria, if it finds one, it will mark '1' in the noise column\n",
    "df.loc[df.elevation >= 4392, 'noise'] = 1\n",
    "\n",
    "#this tells us how many noisy points there are on account of speed or elevation outliers\n",
    "df.loc[df.noise == 1, 'noise'].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3639408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#redo haversine & time difference\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df['distKM'] = \\\n",
    "    haversine(df.latitude.shift(), df.longitude.shift(),\n",
    "              df.loc[1:, 'latitude'], df.loc[1:, 'longitude'])\n",
    "df['distMI'] = df['distKM'] * 0.621371\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800c874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to also redo the time difference between points now that we resampled\n",
    "#make the DateTime_Local the index to run to_series function\n",
    "df.index = df['DateTime_Local']\n",
    "\n",
    "#now calculate distance from previous timepoint to next in hour format\n",
    "df['diff_in_hours'] = df.index.to_series().diff()\n",
    "\n",
    "#calculate new columns for time between points in mins and secs \n",
    "df['durationSecs'] = df['diff_in_hours'].dt.total_seconds()\n",
    "df['durationMins'] = df['durationSecs'] // 60\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58cbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now our last filtering criteria\n",
    "#.0264 is the amount of miles per second a person can travel going 95mph (the max value)\n",
    "#95/3600 = .0264 mp/s\n",
    "\n",
    "y = df.durationSecs * .0264\n",
    "df.loc[df.distMI > y, 'noise'] = 1\n",
    "\n",
    "#so if the distance from one point to the next is greater than possible based on time from one point to the next, it's labeled as noise\n",
    "\n",
    "#check to see how many possible erroneous points there are in our dataset\n",
    "df.loc[df.noise == 1, 'noise'].count()\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d8f110",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset index\n",
    "#df.reset_index(drop=True, inplace=True)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5d7d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if I already ran the code once and have a 1 in the noise column, otherwise need to set noise = 1\n",
    "noise = df['noise'].count().astype(int)\n",
    "\n",
    "noiseList = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "        if noise > 0:\n",
    "            df.reset_index(drop=True, inplace=True)\n",
    "            df['distKM'] = \\\n",
    "            haversine(df.latitude.shift(), df.longitude.shift(),\n",
    "                df.loc[1:, 'latitude'], df.loc[1:, 'longitude'])\n",
    "            df['distMI'] = df['distKM'] * 0.621371\n",
    "            df.index = df['DateTime_Local']\n",
    "        #now calculate distance from previous timepoint to next in hour format\n",
    "            df['diff_in_hours'] = df.index.to_series().diff()\n",
    "        #calculate new columns for time between points in mins and secs\n",
    "            df['durationSecs'] = df['diff_in_hours'].dt.total_seconds()\n",
    "            df['durationMins'] = df['durationSecs'] // 60\n",
    "            maxDist = df.durationSecs * .0264\n",
    "        #if the distance between point A and > B given 95mph, then put 1 in column noise\n",
    "            df.loc[df.distMI > maxDist, 'noise'] = 1\n",
    "            noise = df['noise'].count().astype(int)\n",
    "            noiseList.append(noise)\n",
    "            df.drop(df.loc[df['noise']==1].index, inplace=True)\n",
    "        else:\n",
    "            break\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22062d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#letas count how much noise total we got rid of and call it \n",
    "\n",
    "noiseSum = sum(noiseList)\n",
    "noiseSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15120edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#just rerun the distance and time calculation again to make sure its correct\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df['distKM'] = \\\n",
    "     haversine(df.latitude.shift(), df.longitude.shift(),\n",
    "               df.loc[1:, 'latitude'], df.loc[1:, 'longitude'])\n",
    "df['distMI'] = df['distKM'] * 0.621371\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a3af24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to also redo the time difference between points now that we resampled\n",
    "#make the DateTime_Local the index to run to_series function\n",
    "df.index = df['DateTime_Local']\n",
    "\n",
    "#now calculate distance from previous timepoint to next in hour format\n",
    "df['diff_in_hours'] = df.index.to_series().diff()\n",
    "\n",
    "#calculate new columns for time between points in mins and secs \n",
    "df['durationSecs'] = df['diff_in_hours'].dt.total_seconds()\n",
    "df['durationMins'] = df['durationSecs'] // 60\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47aba20",
   "metadata": {},
   "source": [
    "# Fix Resampling\n",
    "- error in not picking up all the resamples\n",
    "- remove those & redo haversine & time difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a67b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df.durationSecs <20].index, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b31d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to redo the haversine & time diff AGAIN one last time\n",
    "#just rerun the distance and time calculation again to make sure its correct\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df['distKM'] = \\\n",
    "     haversine(df.latitude.shift(), df.longitude.shift(),\n",
    "               df.loc[1:, 'latitude'], df.loc[1:, 'longitude'])\n",
    "df['distMI'] = df['distKM'] * 0.621371\n",
    "\n",
    "\n",
    "#need to also redo the time difference between points now that we resampled\n",
    "#make the DateTime_Local the index to run to_series function\n",
    "df.index = df['DateTime_Local']\n",
    "\n",
    "#now calculate distance from previous timepoint to next in hour format\n",
    "df['diff_in_hours'] = df.index.to_series().diff()\n",
    "\n",
    "#calculate new columns for time between points in mins and secs \n",
    "df['durationSecs'] = df['diff_in_hours'].dt.total_seconds()\n",
    "df['durationMins'] = df['durationSecs'] // 60\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fca90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the file as th3 cleaned file!!!!!\n",
    "df.to_csv(os.path.join(path, \"410002_allpoints_clean.csv\"),index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74458dee",
   "metadata": {},
   "source": [
    "# Hand remove outliers\n",
    "- identify very obvious erroneous datapoints\n",
    "- mark index of item\n",
    "- remove\n",
    "- rerun haversine & distance \n",
    "- if none, skip all of these next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce18e9c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#drop the index column & recount\n",
    "df = df.drop('index', axis = 1)\n",
    "df.insert(1, 'Index', range(1, 1 + len(df)))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9962e6a0",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "- Now lets look at these points in using Folium\n",
    "- Zoom out to look for any outliers and continue to remove\n",
    "- Otherwise inspect to make sure nothing looks off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d6cd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium \n",
    "import requests\n",
    "\n",
    "#starts the map at Iowa City\n",
    "allmap = folium.Map(location=[41.6611, -91.5302], zoom_start=11)\n",
    "\n",
    "#apply df points on the map, using lat, lon rows & Index column as a popup\n",
    "df.apply(lambda row:folium.CircleMarker(location=[row[\"latitude\"], row[\"longitude\"]], radius=5, color='red', popup=row['Index']).add_to(allmap), axis=1)\n",
    "\n",
    "#display map\n",
    "allmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f053f10d",
   "metadata": {},
   "source": [
    "# Hand remove outliers part 2\n",
    "- remove using above map index, mark down in Excel\n",
    "- rerun haversine & distance \n",
    "- if none, skip all of these next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880ccfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the rows with the indexes you mark as outliers based on visual inspection from map\n",
    "#first make Index the index\n",
    "df.set_index('Index', inplace = True)\n",
    "\n",
    "#then drop those specific indexes you noted\n",
    "df.drop([13677, 7132, 7131, 7130, 7129, 13738, 27196, 13728, 13727, 27195, 27194, 27193, 27192, 7133, 27191, 27190, 27189, 27188, 27187, 9042, 9041, 9040, 7326, 7328, 7327, 7325, 16453, 29785, 29784, 29783, 16452, 29782, 16451, 16450, 3589, 16449, 2307, 2306, 13674, 2250, 2249, 2252, 2251, 13675, 13676, 13673], inplace = True)\n",
    "\n",
    "df.reset_index(inplace = True)\n",
    "#then redo the Index colum to recount\n",
    "\n",
    "df['Index'] = range(1, 1 + len(df))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b30dd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize again to make sure all gone\n",
    "\n",
    "#starts the map at Iowa City\n",
    "allmap2 = folium.Map(location=[41.6611, -91.5302], zoom_start=11)\n",
    "\n",
    "#apply df points on the map, using lat, lon rows & Index column as a popup\n",
    "df.apply(lambda row:folium.CircleMarker(location=[row[\"latitude\"], row[\"longitude\"]], radius=5, color='red', popup=row['Index']).add_to(allmap2), axis=1)\n",
    "\n",
    "#display map\n",
    "allmap2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c046acf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if all looks good, rerun haversine & time diff and save file\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df['distKM'] = \\\n",
    "     haversine(df.latitude.shift(), df.longitude.shift(),\n",
    "               df.loc[1:, 'latitude'], df.loc[1:, 'longitude'])\n",
    "df['distMI'] = df['distKM'] * 0.621371\n",
    "\n",
    "\n",
    "#need to also redo the time difference between points now that we resampled\n",
    "#make the DateTime_Local the index to run to_series function\n",
    "df.index = df['DateTime_Local']\n",
    "\n",
    "#now calculate distance from previous timepoint to next in hour format\n",
    "df['diff_in_hours'] = df.index.to_series().diff()\n",
    "\n",
    "#calculate new columns for time between points in mins and secs \n",
    "df['durationSecs'] = df['diff_in_hours'].dt.total_seconds()\n",
    "df['durationMins'] = df['durationSecs'] // 60\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835a0172",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the file as the final cleaned file\n",
    "df.to_csv(os.path.join(path, \"410002_allpoints_clean_final.csv\"),index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f272f3f",
   "metadata": {},
   "source": [
    "# Sleep Points Dataframe\n",
    "- Now we will get a new df with just the sleep points\n",
    "- Perform DBSCAN to get the sleep cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ee6b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets get the participants home location by only selecting the coordinates between 2am-5am and creating a new df with them\n",
    "#set the datetime index and get the points between 2 and 5\n",
    "df.set_index('DateTime_Local', inplace=True)\n",
    "sleep_points = df.between_time('02:00', '05:00')\n",
    "sleep_points.shape\n",
    "\n",
    "#can save to file for double checking and/or doing DBSCAN in ArcGIS\n",
    "#sleep_points.to_csv(os.path.join(path, \"tff_sleep_points.csv\"),index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f56ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize sleep points on map to make sure nothing weird going on\n",
    "sleep_points.reset_index(inplace = True)\n",
    "\n",
    "sleep_points.apply(lambda row:folium.CircleMarker(location=[row[\"latitude\"], row[\"longitude\"]], radius=5, color='blue', popup=row['DateTime_Local']).add_to(allmap2), axis=1)\n",
    "\n",
    "allmap2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9978a2d",
   "metadata": {},
   "source": [
    "# Data Clustering\n",
    "- Reasoning for why DBSCAN performs better than k-means for lat/lon data: https://geoffboeing.com/2014/08/clustering-to-reduce-spatial-data-set-size/\n",
    "\n",
    "- We want to perform DBSCAN on our sleep points data\n",
    "- The idea is to identify the \"home point\" cluster of our data - where participants are located between 2am-5am\n",
    "- Once we identify this cluster using DBSCAN, we will get the median center point of this cluster \n",
    "- This median center home point (in lat,lon) will be compared to our orig. df with a polygon surrounding this coordinate of 50m. Any point within 50m of this polygon will be characterized as \"pt is spending time at home\" and anything outside will be captured as \"pt is spending time outside home\"\n",
    "- Finally we will calculate the proportion of time spent at home versus away from home in the participant's whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1ccbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we just want the lat/lon points of the sleep df converted to a np matrix\n",
    "coords = sleep_points[['latitude', 'longitude']].to_numpy()\n",
    "coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08090745",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules for scatterplots & DBSCAN later\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from geopy.distance import great_circle\n",
    "from shapely.geometry import MultiPoint\n",
    "\n",
    "#run DBSCAN\n",
    "#haversine parameter requires distances are in radians\n",
    "kms_per_radian = 6371.0088\n",
    "\n",
    "#epsilon = the max distance (50 meters or .05 km in this sample) that points can be from each other to be considered a cluster.\n",
    "epsilon = .05 / kms_per_radian\n",
    "\n",
    "#min_samples = min. cluster size (everything else gets classified as noise)\n",
    "#in this dataset the points are collected every 20 secs so must spent at least 1 hr somewhere to be a cluster at home = 3600/20 = 180 points in an hour\n",
    "min_sample = 180\n",
    "\n",
    "db = DBSCAN(eps=epsilon, min_samples=min_sample, algorithm='ball_tree', metric='haversine').fit(np.radians(coords))\n",
    "cluster_labels = db.labels_\n",
    "\n",
    "#number of clusters\n",
    "num_clusters = len(set(cluster_labels))\n",
    "clusters = pd.Series([coords[cluster_labels == n] for n in range(num_clusters)])\n",
    "\n",
    "print('Number of clusters: {}'.format(num_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df817296",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we want to get the point nearest to this cluster's center (cluster 0) and store the lat/long coordinates for later \n",
    "def get_centermost_point(cluster):\n",
    "    centroid = (MultiPoint(cluster).centroid.x, MultiPoint(cluster).centroid.y)\n",
    "    centermost_point = min(cluster, key=lambda point: great_circle(point, centroid).m)\n",
    "    return tuple(centermost_point)\n",
    "\n",
    "#Find the point in each cluster that is closest to its centroid\n",
    "centermost_points = []\n",
    "for cluster in clusters.iteritems():\n",
    "    if len(cluster[1]) >= min_sample:\n",
    "        centermost_points.append(get_centermost_point(cluster[1]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cac9daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip the list of centermost points (lat, lon) tuples into separate lat and lon lists\n",
    "lats, lons = zip(*centermost_points)\n",
    "\n",
    "# from these lats/lons create a new df of one representative point for each cluster (not -1 or outliers)\n",
    "rep_points = pd.DataFrame({'longitude':lons, 'latitude':lats})\n",
    "\n",
    "# pull row from original data set where lat/lon match the lat/lon of each row of representative points\n",
    "#if we wanted to find this point in our overall dataset it makes it easier to see the index & all the other details\n",
    "\n",
    "rs = rep_points.apply(lambda row: df[(df['latitude']==row['latitude']) & (df['longitude']==row['longitude'])].iloc[0], axis=1)\n",
    "\n",
    "#save to file with 6 decimal points\n",
    "rs.to_csv(os.path.join(path, '410002_sleeppoints.csv'), float_format='%.6f', encoding='utf-8')\n",
    "\n",
    "centermost_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cf1337",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the sleep points cluster and the center point on the cluster in a scatterplot to make sure looks correct\n",
    "fig, ax = plt.subplots(figsize=[10, 6])\n",
    "sleep_points_scatter = ax.scatter(sleep_points['longitude'], sleep_points['latitude'], c='purple', edgecolors='black')\n",
    "rs_scatter = ax.scatter(rs['longitude'], rs['latitude'], c='red', edgecolor='black')\n",
    "ax.set_title('Sleep Points before and after reduction')\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.legend([sleep_points_scatter, rs_scatter], ['Sleep Points Unclustered', 'Cluster Center'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9ccf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset index if you need to\n",
    "df.reset_index(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d18607c",
   "metadata": {},
   "source": [
    "# Calculate Basic Metrics\n",
    "- Find how many data points there are per day\n",
    "- Find the total distances travelled per day\n",
    "- Calculate # of points found at home/outside of home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa880f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count how many points there are per day\n",
    "#add a new column in our original (cleaned) df\n",
    "df['count_perday'] = 1\n",
    "\n",
    "df_counts = df.groupby(by=df['DateTime_Local'].dt.date).count()[['count_perday']].reset_index()\n",
    "\n",
    "#also add in a column with participant ID because we need that!\n",
    "df_counts['SubjectID'] = \"410002\"\n",
    "#move it over to the left for prettiness\n",
    "column_to_move = df_counts.pop(\"SubjectID\")\n",
    "df_counts.insert(0, \"SubjectID\", column_to_move)\n",
    "\n",
    "df_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3abd7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#this will tell you how many total distance traveled (in miles) per day\n",
    "df_mi_sum = df.groupby(by=df['DateTime_Local'].dt.date).sum()[['distMI']].reset_index()\n",
    "df_mi_sum\n",
    "\n",
    "#in the future should probably include other filtering logic so that if days have lots of points but normal \"jiggle\" in signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a517eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat this into the new df (called df_counts) where we will store all this participant's data\n",
    "df_counts = pd.merge(df_counts, df_mi_sum, on='DateTime_Local', how='outer')\n",
    "df_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8cf936",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets find the distance FROM that sleep center point to everywhere else by day\n",
    "#lets print out the lat/lon of that point\n",
    "rs\n",
    "\n",
    "#take from it the lat/lon and put in the code below (copy & paste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc36c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rerun haversine from distance from everypoint TO this mean center point\n",
    "#then manually input latitude above for lat1 below and longitude above for lon1 below\n",
    "def haversine(row):\n",
    "    lon1 = -91.535132\n",
    "    lat1 = 41.667101\n",
    "    lon2 = row['longitude']\n",
    "    lat2 = row['latitude']\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(sqrt(a)) \n",
    "    km = 6367 * c\n",
    "    return km \n",
    "\n",
    "#add it to our orig. dataframe so we get this for each point\n",
    "df['distance_from_HomeMI'] = df.apply(lambda row: haversine(row), axis=1) * 0.62137119 #converted to miles by *.62137119, otherwise km\n",
    "df['distance_from_HomeMeter'] = df['distance_from_HomeMI'] * 1609.344 #converted from miles to meters\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a1ebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the max or greatest distance from home per day\n",
    "\n",
    "df_miFromHome_max = df.groupby(df['DateTime_Local'].dt.date)['distance_from_HomeMI'].agg('max').reset_index()\n",
    "df_miFromHome_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc90e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add to the df_counts df\n",
    "df_counts = pd.merge(df_counts, df_miFromHome_max, on='DateTime_Local', how='outer')\n",
    "df_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e38127",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now the last thing to find how many points within 50m of home per day or \"points at home\"\n",
    "#if distance from home <= 50 meters or 0.0310686 miles, then it is inside home, otherwise outside\n",
    "\n",
    "df['at_home'] = np.nan\n",
    "df.loc[df.distance_from_HomeMI <= 0.0310686, 'at_home'] = 1\n",
    "df\n",
    "\n",
    "#save this df and replace the previous allpoints_clean df\n",
    "#NOTE= you need to check if you did visual inspection or not to decide which file name to rewrite (clean or clean_final)\n",
    "df.to_csv(os.path.join(path, \"410002_allpoints_clean_final.csv\"),index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f8e0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now group by day and sum all the points within home!\n",
    "df_points_atHome = df.groupby(by=df['DateTime_Local'].dt.date).sum()[['at_home']].reset_index()\n",
    "df_points_atHome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2423d10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat to the df_counts df\n",
    "df_counts = pd.merge(df_counts, df_points_atHome, on='DateTime_Local', how='outer')\n",
    "df_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fc3703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lastly, calculate % of time spent at home per day\n",
    "df_counts['%time_at_home'] = ((df_counts['at_home']) / (df_counts['count_perday'])) *100\n",
    "\n",
    "#rename all the columns names so they are easier to understand\n",
    "df_counts.rename({\"distMI\": \"Total_DistMI_SUM\", \n",
    "           \"distance_from_HomeMI\": \"MAX_distancefromhome_MI\"}, \n",
    "          axis = \"columns\", inplace = True)\n",
    "#save this excel as the final metrics for this participant\n",
    "df_counts.to_csv(os.path.join(path, '410002_totalcounts.csv'))\n",
    "\n",
    "df_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7304caa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the average of time spent at home across all days\n",
    "df_counts['AVG_%time_at_home'] = df_counts.loc[:, '%time_at_home'].mean()\n",
    "\n",
    "df_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efa38bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the average distances travelled \n",
    "df_counts['AVG_disMI'] = df_counts.loc[:, 'Total_DistMI_SUM'].mean()\n",
    "\n",
    "df_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affbb539",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save this excel as the final metrics for this participant\n",
    "df_counts.to_csv(os.path.join(path, '410002_totalcounts.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd46611f",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
